<overview>TDD is about design quality, not coverage metrics. The red-green-refactor cycle forces you to think about behavior before implementation, producing cleaner interfaces and more testable code.**Principle:** If you can describe the behavior as `expect(fn(input)).toBe(output)` before writing `fn`, TDD improves the result.**Key insight:** TDD work is fundamentally heavier than standard tasks—it requires 2-3 execution cycles (RED → GREEN → REFACTOR), each with file reads, test runs, and potential debugging. TDD features get dedicated plans to ensure full context is available throughout the cycle.</overview><when_to_use_tdd>## When TDD Improves Quality**TDD candidates (create a TDD plan):**- Business logic with defined inputs/outputs- API endpoints with request/response contracts- Data transformations, parsing, formatting- Validation rules and constraints- Algorithms with testable behavior- State machines and workflows- Utility functions with clear specifications**Skip TDD (use standard plan with `type="auto"` tasks):**- UI layout, styling, visual components- Configuration changes- Glue code connecting existing components- One-off scripts and migrations- Simple CRUD with no business logic- Exploratory prototyping**Heuristic:** Can you write `expect(fn(input)).toBe(output)` before writing `fn`?→ Yes: Create a TDD plan→ No: Use standard plan, add tests after if needed</when_to_use_tdd><tdd_plan_structure>## TDD Plan StructureEach TDD plan implements **one feature** through the full RED-GREEN-REFACTOR cycle.```markdown---phase: XX-nameplan: NNtype: tdd---<objective>[What feature and why]Purpose: [Design benefit of TDD for this feature]Output: [Working, tested feature]</objective><context>@.planning/PROJECT.md@.planning/ROADMAP.md@relevant/source/files.ts</context><feature>  <name>[Feature name]</name>  <files>[source file, test file]</files>  <behavior>    [Expected behavior in testable terms]    Cases: input → expected output  </behavior>  <implementation>[How to implement once tests pass]</implementation></feature><verification>[Test command that proves feature works]</verification><success_criteria>- Failing test written and committed- Implementation passes test- Refactor complete (if needed)- All 2-3 commits present</success_criteria><output>After completion, create SUMMARY.md with:- RED: What test was written, why it failed- GREEN: What implementation made it pass- REFACTOR: What cleanup was done (if any)- Commits: List of commits produced</output>```html```bash# JavaScript/TypeScriptif [ -f package.json ]; then echo "node"; fi# Pythonif [ -f requirements.txt ] || [ -f pyproject.toml ]; then echo "python"; fi# Goif [ -f go.mod ]; then echo "go"; fi# Rustif [ -f Cargo.toml ]; then echo "rust"; fi```text```bash# Run empty test suite - should pass with 0 testsnpm test  # Nodepytest    # Pythongo test ./...  # Gocargo test    # Rust```markdown```markdown
```**Comparison with standard plans:**- Standard plans: 1 commit per task, 2-4 commits per plan- TDD plans: 2-3 commits for single featureBoth follow same format: `{type}({phase}-{plan}): {description}`**Benefits:**- Each commit independently revertable- Git bisect works at commit level- Clear history showing TDD discipline- Consistent with overall commit strategy</commit_pattern><context_budget>## Context BudgetTDD plans target **~40% context usage** (lower than standard plans' ~50%).Why lower:- RED phase: write test, run test, potentially debug why it didn't fail- GREEN phase: implement, run test, potentially iterate on failures- REFACTOR phase: modify code, run tests, verify no regressionsEach phase involves reading files, running commands, analyzing output. The back-and-forth is inherently heavier than linear task execution.Single feature focus ensures full quality throughout the cycle.</context_budget>