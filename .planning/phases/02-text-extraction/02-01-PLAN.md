---
phase: 02-text-extraction
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/db/index.ts
  - services/contract/extraction/types.ts
  - services/contract/extraction/storage.ts
  - services/contract/extraction/digital-extractor.ts
  - services/contract/extraction/ocr-extractor.ts
  - services/contract/extraction/text-extractor.ts
  - services/contract/pipeline/dedup.ts
autonomous: true
user_setup:
  - service: mistral
    why: "OCR for scanned PDF extraction"
    env_vars:
      - name: MISTRAL_API_KEY
        source: "Mistral Console -> API Keys -> Create new key (https://console.mistral.ai/api-keys/)"

must_haves:
  truths:
    - "Digital PDFs have text extracted accurately"
    - "Scanned PDFs are OCR'd using Mistral and text is readable"
    - "Extracted text is stored per-page and available for multi-agent processing"
  artifacts:
    - path: "services/contract/extraction/types.ts"
      provides: "ExtractedPage and ExtractionResult type definitions"
      exports: ["ExtractedPage", "ExtractionResult"]
    - path: "services/contract/extraction/storage.ts"
      provides: "SQLite storage for extracted pages"
      exports: ["storeExtractedPages", "getExtractedPages", "getFullText"]
    - path: "services/contract/extraction/digital-extractor.ts"
      provides: "pdfjs-dist text extraction for digital PDFs"
      exports: ["extractDigitalText"]
    - path: "services/contract/extraction/ocr-extractor.ts"
      provides: "Mistral OCR extraction for scanned PDFs"
      exports: ["extractWithOcr"]
    - path: "services/contract/extraction/text-extractor.ts"
      provides: "Main orchestrator with two-tier extraction strategy"
      exports: ["extractText"]
    - path: "lib/db/index.ts"
      provides: "contract_pages table schema"
      contains: "CREATE TABLE IF NOT EXISTS contract_pages"
  key_links:
    - from: "services/contract/extraction/text-extractor.ts"
      to: "services/contract/extraction/digital-extractor.ts"
      via: "import and call extractDigitalText first"
      pattern: "extractDigitalText"
    - from: "services/contract/extraction/text-extractor.ts"
      to: "services/contract/extraction/ocr-extractor.ts"
      via: "fallback call if text yield < 100 chars/page"
      pattern: "extractWithOcr"
    - from: "services/contract/extraction/storage.ts"
      to: "lib/db/index.ts"
      via: "db import for contract_pages operations"
      pattern: "from.*@/lib/db"
---

<objective>
Implement two-tier text extraction for contract PDFs: try fast local extraction (pdfjs-dist) first, fall back to Mistral OCR for scanned PDFs. Store extracted text per-page in SQLite for downstream multi-agent processing with citation support.

Purpose: Enable Phase 3 extraction agents to access contract text with page-level citations for human verification.

Output: Working text extraction pipeline that handles both digital and scanned PDFs, with per-page storage in SQLite.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-text-extraction/02-RESEARCH.md
@.planning/phases/01-pipeline-foundation/01-01-SUMMARY.md
@services/contract/pipeline/types.ts
@services/contract/pipeline/dedup.ts
@lib/db/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create types, schema, and storage layer</name>
  <files>
    services/contract/extraction/types.ts
    services/contract/extraction/storage.ts
    lib/db/index.ts
    services/contract/pipeline/dedup.ts
  </files>
  <action>
    1. Create `services/contract/extraction/types.ts` with:
       - `ExtractedPage` type: { pageIndex: number, text: string, source: "digital" | "ocr" }
       - `ExtractionResult` type: { pages: ExtractedPage[], totalPages: number, extractionMethod: "digital" | "ocr" | "hybrid", processingTimeMs: number }

    2. Add `contract_pages` table to `lib/db/index.ts` (after processed_contracts section):
       ```sql
       CREATE TABLE IF NOT EXISTS contract_pages (
         id INTEGER PRIMARY KEY AUTOINCREMENT,
         contract_id INTEGER NOT NULL,
         page_index INTEGER NOT NULL,
         text TEXT NOT NULL,
         source TEXT NOT NULL DEFAULT 'digital',
         extracted_at TEXT NOT NULL DEFAULT (datetime('now')),
         FOREIGN KEY (contract_id) REFERENCES processed_contracts(id) ON DELETE CASCADE,
         UNIQUE(contract_id, page_index)
       )
       ```
       Add index: `CREATE INDEX IF NOT EXISTS idx_contract_pages_contract ON contract_pages(contract_id)`

    3. Create `services/contract/extraction/storage.ts` with:
       - `storeExtractedPages(contractId: number, pages: ExtractedPage[]): void` - INSERT OR REPLACE pages
       - `getExtractedPages(contractId: number): ExtractedPage[]` - SELECT pages ordered by page_index
       - `getFullText(contractId: number): string` - Join all pages with "\n\n---PAGE BREAK---\n\n"

    4. Update `services/contract/pipeline/dedup.ts`:
       - Modify `markAsProcessed` to RETURN the contract ID (use `db.query(...).get()` with RETURNING clause or run INSERT then SELECT last_insert_rowid())
       - Signature: `markAsProcessed(filePath: string, status?: ProcessingStatus): number`
  </action>
  <verify>
    Run `bun -e "import { db } from './lib/db'; console.log(db.query('SELECT sql FROM sqlite_master WHERE name = \"contract_pages\"').get())"` - should show table schema.
    Run `bunx tsc --noEmit services/contract/extraction/types.ts services/contract/extraction/storage.ts` - no type errors.
  </verify>
  <done>
    - ExtractedPage and ExtractionResult types defined and exported
    - contract_pages table exists in SQLite schema
    - storeExtractedPages, getExtractedPages, getFullText functions work
    - markAsProcessed returns contract ID for linking pages
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement digital and OCR extractors</name>
  <files>
    services/contract/extraction/digital-extractor.ts
    services/contract/extraction/ocr-extractor.ts
    package.json
  </files>
  <action>
    1. Install Mistral SDK: `bun add @mistralai/mistralai`

    2. Create `services/contract/extraction/digital-extractor.ts`:
       - Export `extractDigitalText(filePath: string): Promise<ExtractionResult>`
       - Use existing `pdfjs-dist` (already installed) for extraction
       - Load PDF: `const loadingTask = getDocument(new Uint8Array(buffer))`
       - Extract per-page: iterate `pdf.numPages`, call `page.getTextContent()`, join items
       - Return ExtractionResult with source: "digital" for each page
       - Handle errors gracefully (return empty pages array on failure)

       Note: pdfjs-dist v5 API - use `getDocument()` from "pdfjs-dist/legacy/build/pdf.mjs" for Bun compatibility.

    3. Create `services/contract/extraction/ocr-extractor.ts`:
       - Export `extractWithOcr(filePath: string, startTime?: number): Promise<ExtractionResult>`
       - Initialize Mistral client with `process.env.MISTRAL_API_KEY`
       - Read file as base64: `Buffer.from(buffer).toString("base64")`
       - Call `mistral.ocr.process()` with:
         - model: "mistral-ocr-latest"
         - document: { type: "document_url", documentUrl: `data:application/pdf;base64,${base64}` }
         - includeImageBase64: false
         - tableFormat: "markdown" (preserves table structure)
       - Map response.pages to ExtractedPage[] with source: "ocr"
       - Handle API errors (throw with descriptive message)

    Import types from "./types.ts" in both files.
  </action>
  <verify>
    Run `bunx tsc --noEmit services/contract/extraction/digital-extractor.ts services/contract/extraction/ocr-extractor.ts` - no type errors.
    Run `bun -e "import { extractDigitalText } from './services/contract/extraction/digital-extractor'"` - imports without error.
  </verify>
  <done>
    - @mistralai/mistralai added to dependencies
    - extractDigitalText extracts text from digital PDFs using pdfjs-dist
    - extractWithOcr calls Mistral OCR API for scanned PDFs
    - Both return ExtractionResult with per-page text and source tracking
  </done>
</task>

<task type="auto">
  <name>Task 3: Create main orchestrator and wire to pipeline</name>
  <files>
    services/contract/extraction/text-extractor.ts
    services/contract/pipeline/index.ts
  </files>
  <action>
    1. Create `services/contract/extraction/text-extractor.ts`:
       - Export `extractText(filePath: string): Promise<ExtractionResult>`
       - Implement two-tier extraction strategy:
         a. Call `extractDigitalText(filePath)` first
         b. Calculate average chars per page
         c. If avgCharsPerPage < 100 (likely scanned), call `extractWithOcr(filePath, startTime)`
         d. Otherwise return digital extraction result
       - Track total processing time (Date.now() at start)
       - Log extraction method used (digital vs ocr) to console

       Threshold constant: `const MIN_CHARS_PER_PAGE = 100`

    2. Update `services/contract/pipeline/index.ts` to integrate extraction:
       - Import `extractText` from "../extraction/text-extractor"
       - Import `storeExtractedPages` from "../extraction/storage"
       - Import `markAsProcessed`, `updateProcessingStatus` from "./dedup"
       - Update the handler function to:
         a. Call `markAsProcessed(filePath, "processing")` - get contractId
         b. Call `extractText(filePath)` - get ExtractionResult
         c. Call `storeExtractedPages(contractId, result.pages)` - persist
         d. Call `updateProcessingStatus(filePath, "completed")`
         e. Wrap in try/catch - on error, call `updateProcessingStatus(filePath, "failed")`
         f. Log: "Extracted {N} pages via {method} in {ms}ms"
  </action>
  <verify>
    Run `bunx tsc --noEmit services/contract/extraction/text-extractor.ts services/contract/pipeline/index.ts` - no type errors.
    Run `bun x ultracite check` - no lint errors.
    Manual test: Create a test PDF, drop in watch folder, check contract_pages table has rows.
  </verify>
  <done>
    - extractText orchestrates digital-first extraction with OCR fallback
    - Pipeline handler integrates extraction: detect -> extract -> store -> update status
    - Extracted pages stored in contract_pages with contract_id foreign key
    - Processing status tracked (processing -> completed/failed)
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Schema verification:**
   ```bash
   bun -e "import { db } from './lib/db'; console.log(db.query('SELECT name FROM sqlite_master WHERE type=\"table\" AND name=\"contract_pages\"').get())"
   ```
   Expected: { name: 'contract_pages' }

2. **Type check:**
   ```bash
   bunx tsc --noEmit
   ```
   Expected: No errors

3. **Lint check:**
   ```bash
   bun x ultracite check
   ```
   Expected: No errors

4. **Import verification:**
   ```bash
   bun -e "import { extractText } from './services/contract/extraction/text-extractor'; import { storeExtractedPages, getExtractedPages, getFullText } from './services/contract/extraction/storage'; console.log('All exports available')"
   ```
   Expected: "All exports available"

5. **Integration test (if test PDF available):**
   - Place a digital PDF in watch folder
   - Check `processed_contracts` table has entry with status "completed"
   - Check `contract_pages` table has rows for that contract_id
</verification>

<success_criteria>
Phase 2 is complete when:
- Digital PDFs have text extracted using pdfjs-dist (no API call needed)
- Scanned PDFs (< 100 chars/page avg) are OCR'd using Mistral API
- Extracted text is stored per-page in contract_pages table
- Each page has source tracking ("digital" or "ocr")
- Pipeline integration works: detect -> extract -> store -> status update
- All code passes type check and lint
</success_criteria>

<output>
After completion, create `.planning/phases/02-text-extraction/02-01-SUMMARY.md`
</output>
